{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNt971dVeXqXlO1JeaPGktY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Liza-b-13/liza-repo/blob/main/notebook/breaking_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Adversarial machine learning algorithms**\n",
        "\n",
        "> Adversarial machine learning est l'art d'étudier comment casser et sécuriser les modèles d'apprentissage automatique"
      ],
      "metadata": {
        "id": "nvnv4cmI7s1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Il y a deux types d'attaques adversariales:**\n",
        "\n",
        "->**Attaque de type I** : faire plus de faux positifs (faire passer des malwares comme benin)\n",
        "\n",
        "*   Il faut vraiment beaucoup modifier la donnée pour la faire passer comme saine\n",
        "*   On utilise en general des GAN et autoencodeurs pour y arriver\n",
        "\n",
        "\n",
        "->**Attaque de type II** : faire plus de faux négatifs (faire passer des benins comme malware)  \n",
        "*   Cette attaque est plus facile, car il suffit d'ajouter du bruit aux données saines pour les rendres non reconnaissables par l'algo\n",
        "*   des toutes petites modifications en general\n",
        "\n"
      ],
      "metadata": {
        "id": "QB4C6PPp-bkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Différent types d'attaques sur ML\n",
        "\n",
        "### Attaques d'évasion\n",
        "Les attaques d'évasion visent à contourner un modèle ML en observant ses sorties et en testant de nombreux exemples pour découvrir les motifs de décision. L'attaquant nourrit le modèle avec des entrées variées, identifie les modifications minimales qui font passer un exemple malveillant pour bénin (ex. : faire passer un spam comme non-spam) et génère des exemples adversariaux qui contournent la détection.\n",
        "\n",
        "### Attaques par empoisonnement\n",
        "Les attaques par empoisonnement visent à corrompre le processus d'apprentissage en injectant des données malveillantes durant la phase d'entraînement, pour faire dévier le modèle et obtenir des résultats souhaités par l'attaquant (ex. : rendre un IDS moins efficace). L'attaque se déroule typiquement lorsque la collecte de données est possible en ligne et que des échantillons manipulés peuvent être ajoutés au jeu d'entraînement.\n",
        "(en gros on apprend des donnés fausse au ML et il va etre moins fiable car il est entrainé sur des données pièges)\n",
        "\n",
        "### Adversarial clustering\n",
        "L’adversarial clustering consiste à attaquer les algorithmes de regroupement (clustering) en modifiant ou en ajoutant quelques données pour perturber la formation des clusters.\n",
        "L’attaquant insère un petit nombre d’échantillons malveillants qui se fondent dans les groupes existants afin de cacher des données anormales ou de fausser la structure des clusters.\n",
        "\n",
        "But : tromper le modèle non supervisé pour qu’il considère des données malveillantes comme normales.\n",
        "\n",
        "**et autres...**"
      ],
      "metadata": {
        "id": "RnwGfHcw3TAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Nous allonrs créer un petit example d'attaque par empoisonnement:\n",
        "\n",
        "Nous allons utiliser la méthode **JSMA (Jacobian-Based Saliency Map Attack)**"
      ],
      "metadata": {
        "id": "x_SfB_Mj70MJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XAu1WWg27z5v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}